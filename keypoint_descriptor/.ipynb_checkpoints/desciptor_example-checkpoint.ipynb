{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE pretrained network to output keypoint's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptor_CNN3 import DesNet\n",
    "model = DesNet()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "weight_path = \"checkpoint.pth\"\n",
    "trained_weight = torch.load(weight_path)\n",
    "model.load_state_dict(trained_weight['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 1, 32, 32])\n",
      "torch.Size([35, 30, 1, 32, 32])\n",
      "torch.Size([175, 30, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# load patches\n",
    "patches_dir_images = \"../keypoint_detector/patches_images.pt\"\n",
    "patches_dir_query = \"../keypoint_detector/patches_query.pt\"\n",
    "patches_dir_all = \"../keypoint_detector/patches_all.pt\"\n",
    "patches_images = torch.load(patches_dir_images)\n",
    "patches_query = torch.load(patches_dir_query)\n",
    "patches_all = torch.load(patches_dir_all)\n",
    "\n",
    "print(patches_images.shape)\n",
    "print(patches_query.shape)\n",
    "print(patches_all.shape)\n",
    "\n",
    "patches_query =  patches_query.view(-1, 1, 32, 32).cuda()\n",
    "patches_images =  patches_images.view(-1, 1, 32, 32).cuda()\n",
    "patches_all =  patches_all.view(-1, 1, 32, 32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.50 MiB (GPU 0; 10.92 GiB total capacity; 592.99 MiB already allocated; 14.50 MiB free; 783.00 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec5eef8ff4d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdescription_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdescription_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescription_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdescription_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/massonm/CNN_keypoints_description/MatchingKeypoints_ImageRetrieval/keypoint_descriptor/descriptor_CNN3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mL2Norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/massonm/CNN_keypoints_description/MatchingKeypoints_ImageRetrieval/keypoint_descriptor/descriptor_CNN3.py\u001b[0m in \u001b[0;36minput_norm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.50 MiB (GPU 0; 10.92 GiB total capacity; 592.99 MiB already allocated; 14.50 MiB free; 783.00 KiB cached)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    description_images = model(patches_images)\n",
    "    description_images = description_images.view(-1, 30, 128).cpu().data\n",
    "    description_query = model(patches_query)\n",
    "    description_query = description_query.view(-1, 30, 128).cpu().data\n",
    "    description_all = model(patches_all)\n",
    "    description_all = description_all.view(-1, 30, 128).cpu().data\n",
    "\n",
    "    print(description_images.shape)\n",
    "    print(description_query.shape)\n",
    "    print(description_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save deep features  \n",
    "# IMAGES\n",
    "output_dir_images = \"images_keypoints_descriptions.pt\"\n",
    "torch.save(description_images, output_dir_images)\n",
    "\n",
    "# QUERY\n",
    "output_dir_query = \"query_keypoints_descriptions.pt\"\n",
    "torch.save(description_query, output_dir_query)\n",
    "\n",
    "# QUERY + IMAGES\n",
    "output_dir_query_and_images = \"query_and_images_keypoints_descriptions.pt\"\n",
    "torch.save(description_all, output_dir_query_and_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 30, 128])\n",
      "torch.Size([140, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "# Load descriptions of the images\n",
    "images_description = torch.load(output_dir_images)\n",
    "query_description = torch.load(output_dir_query)\n",
    "query_and_images_description = torch.load(output_dir_query_and_images)\n",
    "\n",
    "print(query_description.shape)\n",
    "print(images_description.shape)\n",
    "print(query_and_images_description.shape)\n",
    "\n",
    "#print(query_description)\n",
    "#print(query_and_images_description)\n",
    "#print(images_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one keypoint matching: Compute the cost matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "sim_matrix = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "        # Hungarian: one-to-one matching\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(np.copy(cost_matrix))\n",
    "        \n",
    "        for m in indexes:\n",
    "                sim_matrix[qkeypoint, images_keypoint] += np.exp(-cost_matrix[m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.09582365 15.95460675 15.27731852 ...  8.89354472  8.96293825\n",
      "   8.82521085]\n",
      " [ 9.52089546  9.51881673  9.47644828 ...  9.50603487  9.55114158\n",
      "   9.23158531]\n",
      " [ 9.74975131  9.79641903  9.62664098 ...  9.076581    9.04206852\n",
      "   9.21711886]\n",
      " ...\n",
      " [ 8.86574528  8.90869303  8.75763617 ...  9.09690092  9.06312987\n",
      "   9.38135474]\n",
      " [ 8.96228402  9.13603875  8.76421999 ...  9.02715686  9.07895744\n",
      "   9.10371407]\n",
      " [ 9.13884786  8.9311002   8.84456485 ... 10.18426631 17.81534871\n",
      "  11.70028291]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "one_to_one_similitude_tensor = torch.as_tensor(sim_matrix)\n",
    "\n",
    "print(one_to_one_similitude_tensor.shape)\n",
    "print(one_to_one_similitude_tensor)\n",
    "torch.save(one_to_one_similitude_tensor, \"one_to_one_similitude_matrix.pt\")\n",
    "test = torch.load(\"one_to_one_similitude_matrix.pt\")\n",
    "print(test.shape)\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to many matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_many_to_many = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "\n",
    "        sim_matrix = np.exp(-cost_matrix)\n",
    "        x = sim_matrix/ np.linalg.norm(sim_matrix, axis=0)\n",
    "        sim_matrix_many_to_many[qkeypoint][images_keypoint] = np.multiply(sim_matrix, x).sum()\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"many_to_many_similitude_matrix\", sim_matrix_many_to_many, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51.7252415  50.62483538 51.34427346 ... 44.00673066 43.55393212\n",
      "  43.21341456]\n",
      " [46.51604753 46.30991434 46.43907204 ... 47.23570939 46.66816236\n",
      "  45.44350613]\n",
      " [45.06540521 45.20175599 45.25944013 ... 44.84995313 44.51627118\n",
      "  44.35296605]\n",
      " ...\n",
      " [43.30583658 43.58107569 43.37359059 ... 44.64019902 44.25281825\n",
      "  44.79963042]\n",
      " [44.18130976 44.43467499 43.74936925 ... 44.90966726 44.62806266\n",
      "  44.80982697]\n",
      " [44.51699584 43.99887962 43.8592617  ... 48.92385104 52.4003462\n",
      "  47.40526241]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#print(sim_matrix_many_to_many[1])\n",
    "print(sim_matrix_many_to_many)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "sim_matrix_many_to_many_tensor = torch.as_tensor(sim_matrix_many_to_many)\n",
    "\n",
    "print(sim_matrix_many_to_many_tensor.shape)\n",
    "print(sim_matrix_many_to_many_tensor)\n",
    "torch.save(sim_matrix_many_to_many_tensor, \"many_to_many_similitude_matrix.pt\")\n",
    "test2 = torch.load(\"many_to_many_similitude_matrix.pt\")\n",
    "print(test2.shape)\n",
    "print(test2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 140])\n",
      "140\n",
      "['q1' 'q1' 'q1' 'q1' 'q2' 'q2' 'q2' 'q2' 'q3' 'q3' 'q3' 'q3' 'q4' 'q4'\n",
      " 'q4' 'q4' 'q5' 'q5' 'q5' 'q5' 'q6' 'q6' 'q6' 'q6' 'q7' 'q7' 'q7' 'q7'\n",
      " 'q8' 'q8' 'q8' 'q8' 'q9' 'q9' 'q9' 'q9' 'q10' 'q10' 'q10' 'q10' 'q11'\n",
      " 'q11' 'q11' 'q11' 'q12' 'q12' 'q12' 'q12' 'q13' 'q13' 'q13' 'q13' 'q14'\n",
      " 'q14' 'q14' 'q14' 'q15' 'q15' 'q15' 'q15' 'q16' 'q16' 'q16' 'q16' 'q17'\n",
      " 'q17' 'q17' 'q17' 'q18' 'q18' 'q18' 'q18' 'q19' 'q19' 'q19' 'q19' 'q20'\n",
      " 'q20' 'q20' 'q20' 'q21' 'q21' 'q21' 'q21' 'q22' 'q22' 'q22' 'q22' 'q23'\n",
      " 'q23' 'q23' 'q23' 'q24' 'q24' 'q24' 'q24' 'q25' 'q25' 'q25' 'q25' 'q26'\n",
      " 'q26' 'q26' 'q26' 'q27' 'q27' 'q27' 'q27' 'q28' 'q28' 'q28' 'q28' 'q29'\n",
      " 'q29' 'q29' 'q29' 'q30' 'q30' 'q30' 'q30' 'q31' 'q31' 'q31' 'q31' 'q32'\n",
      " 'q32' 'q32' 'q32' 'q33' 'q33' 'q33' 'q33' 'q34' 'q34' 'q34' 'q34' 'q35'\n",
      " 'q35' 'q35' 'q35']\n",
      "140\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate keypoint matching by estimating precision and recall of image retrieval\n",
    "\n",
    "print(one_to_one_similitude_matrix.shape)\n",
    "queryIDs, indexes_sim_matrix = np.loadtxt(\"../image_retrieval/ground_truth.txt\", dtype={'names': ['label', 'age'],'formats': ('U10', 'i4')}, usecols =(0, 1), unpack = True)\n",
    "\n",
    "print(queryIDs.size)\n",
    "print(queryIDs)\n",
    "print(indexes_sim_matrix.size)\n",
    "print(indexes_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_to_one_similitude_matrix = torch.load(\"results/one_to_one_similitude_matrix.pt\")\n",
    "many_to_many_similitude_matrix = torch.load(\"results/many_to_many_similitude_matrix.pt\")\n",
    "\n",
    "# K = most similar images\n",
    "K = 2\n",
    "K_one_to_one = np.zeros((35,K))\n",
    "K_many_to_many = np.zeros((35,K))\n",
    "#print(K)\n",
    "id = 0\n",
    "for i in range(queryIDs.size):\n",
    "    #print(queryIDs[i])\n",
    "    for queryID in range(36):\n",
    "        #id = 0\n",
    "        if queryIDs[i] == 'q'+ str(queryID):\n",
    "           # print(\"hello \" + 'q'+  str(queryID))\n",
    "            m = indexes_sim_matrix[i]\n",
    "           # print(m)\n",
    "            sim_one_to_one = one_to_one_similitude_matrix[queryID-1][i].data.cpu().numpy()\n",
    "            sim_many_to_many = many_to_many_similitude_matrix[queryID-1][i].data.cpu().numpy()\n",
    "            #print(sim)\n",
    "            K_one_to_one[queryID-1][id] = sim_one_to_one\n",
    "            K_many_to_many[queryID-1][id] = sim_many_to_many\n",
    "            id += 1\n",
    "        if id == K:\n",
    "            id = 0\n",
    "            \n",
    "#print(K_one_to_one.size)\n",
    "#print(K_one_to_one)\n",
    "\n",
    "#print(K_many_to_many.size)\n",
    "#print(K_many_to_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine true or false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nmaxelements(list1, N): \n",
    "    l = list1.copy()\n",
    "    final_list = []\n",
    "  \n",
    "    for i in range(0, N):  \n",
    "        max1 = 0\n",
    "          \n",
    "        for j in range(len(list1)):      \n",
    "            if list1[j] > max1: \n",
    "                max1 = list1[j];\n",
    "                list1[j] = 0;\n",
    "               # print(l.index(max1))\n",
    "                  \n",
    "        #list1.remove(max1); \n",
    "        #final_list[l.index(max1)+1] = max1 \n",
    "        final_list.append(max1)\n",
    "          \n",
    "    #print(final_list) \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE [0.3]\n"
     ]
    }
   ],
   "source": [
    "one_to_one_similitude_matrix = torch.load(\"results/one_to_one_similitude_matrix.pt\")\n",
    "one_to_one_similitude_matrix = one_to_one_similitude_matrix.numpy()\n",
    "\n",
    "highest_matching_points = np.zeros((35,K))\n",
    "true_positives = np.zeros((35,K))\n",
    "precision_matrix = np.zeros((35,1))\n",
    "\n",
    "for i in range(35):\n",
    "        highest_matching_points[i] = Nmaxelements(one_to_one_similitude_matrix[i], K)\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "for i in range(35):\n",
    "    K_one_to_one[i].sort();\n",
    "    highest_matching_points[i].sort()\n",
    "    total_true_positives = 0\n",
    "    for j in range(K):\n",
    "        if K_one_to_one[i][j] == highest_matching_points[i][j]:\n",
    "            true_positives[i][j] = 1.\n",
    "            total_true_positives += 1\n",
    "            #print(K_one_to_one[i][j])\n",
    "        else:\n",
    "            true_positives[i][j] = 0.\n",
    "            #print(K_one_to_one[i][j])\n",
    "            #print(highest_matching_points[i][j])\n",
    "    precision_matrix[i] = total_true_positives/K\n",
    "\n",
    "    \n",
    "#print(precision_matrix)\n",
    "average_precision = 0\n",
    "for i in range(35):\n",
    "    average_precision += precision_matrix[i]\n",
    "\n",
    "average_precision = average_precision / 35\n",
    "print(\"AVERAGE\", average_precision)\n",
    "#print(K_one_to_one)\n",
    "#print(highest_matching_points)\n",
    "#print(true_positives)\n",
    "#print(total_true_positives)\n",
    "#precision = total_true_positives / K\n",
    "\n",
    "#print(precision)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION AVERAGE [0.22857143]\n"
     ]
    }
   ],
   "source": [
    "many_to_many_similitude_matrix = torch.load(\"results/many_to_many_similitude_matrix.pt\")\n",
    "many_to_many_similitude_matrix = many_to_many_similitude_matrix.numpy()\n",
    "\n",
    "highest_matching_points_many_to_many = np.zeros((35,K))\n",
    "true_positives_many_to_many = np.zeros((35,K))\n",
    "precision_matrix_many_to_many = np.zeros((35,1))\n",
    "\n",
    "for i in range(35):\n",
    "        highest_matching_points_many_to_many[i] = Nmaxelements(many_to_many_similitude_matrix[i], K)\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "for i in range(35):\n",
    "    K_many_to_many[i].sort();\n",
    "    highest_matching_points_many_to_many[i].sort()\n",
    "    total_true_positives_many_to_many = 0\n",
    "    for j in range(K):\n",
    "        if K_many_to_many[i][j] == highest_matching_points_many_to_many[i][j]:\n",
    "            true_positives_many_to_many[i][j] = 1.\n",
    "            total_true_positives_many_to_many += 1\n",
    "            #print(K_one_to_one[i][j])\n",
    "        else:\n",
    "            true_positives_many_to_many[i][j] = 0.\n",
    "            #print(K_one_to_one[i][j])\n",
    "            #print(highest_matching_points[i][j])\n",
    "    precision_matrix_many_to_many[i] = total_true_positives_many_to_many/K\n",
    "\n",
    "    \n",
    "#print(precision_matrix_many_to_many)\n",
    "average_precision = 0\n",
    "for i in range(35):\n",
    "    average_precision += precision_matrix_many_to_many[i]\n",
    "\n",
    "average_precision = average_precision / 35\n",
    "print(\"PRECISION AVERAGE\", average_precision)\n",
    "#print(K_one_to_one)\n",
    "#print(highest_matching_points)\n",
    "#print(true_positives_many_to_many)\n",
    "#print(total_true_positives_many_to_many)\n",
    "#precision = total_true_positives / K\n",
    "\n",
    "#print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
