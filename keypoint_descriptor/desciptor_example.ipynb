{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE pretrained network to output keypoint's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptor_CNN3 import DesNet\n",
    "model = DesNet()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "weight_path = \"checkpoint.pth\"\n",
    "trained_weight = torch.load(weight_path)\n",
    "model.load_state_dict(trained_weight['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 1, 32, 32])\n",
      "torch.Size([35, 30, 1, 32, 32])\n",
      "torch.Size([175, 30, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# load patches\n",
    "patches_dir_images = \"../keypoint_detector/patches_images.pt\"\n",
    "patches_dir_query = \"../keypoint_detector/patches_query.pt\"\n",
    "patches_dir_all = \"../keypoint_detector/patches_all.pt\"\n",
    "patches_images = torch.load(patches_dir_images)\n",
    "patches_query = torch.load(patches_dir_query)\n",
    "patches_all = torch.load(patches_dir_all)\n",
    "\n",
    "print(patches_images.shape)\n",
    "print(patches_query.shape)\n",
    "print(patches_all.shape)\n",
    "\n",
    "patches_query =  patches_query.view(-1, 1, 32, 32).cuda()\n",
    "patches_images =  patches_images.view(-1, 1, 32, 32).cuda()\n",
    "patches_all =  patches_all.view(-1, 1, 32, 32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 128])\n",
      "torch.Size([35, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    description_images = model(patches_images)\n",
    "    description_images = description_images.view(-1, 30, 128).cpu().data\n",
    "    description_query = model(patches_query)\n",
    "    description_query = description_query.view(-1, 30, 128).cpu().data\n",
    "    description_all = model(patches_all)\n",
    "    description_all = description_all.view(-1, 30, 128).cpu().data\n",
    "\n",
    "    print(description_images.shape)\n",
    "    print(description_query.shape)\n",
    "    print(description_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save deep features  \n",
    "# IMAGES\n",
    "output_dir_images = \"images_keypoints_descriptions.pt\"\n",
    "torch.save(description_images, output_dir_images)\n",
    "\n",
    "# QUERY\n",
    "output_dir_query = \"query_keypoints_descriptions.pt\"\n",
    "torch.save(description_query, output_dir_query)\n",
    "\n",
    "# QUERY + IMAGES\n",
    "output_dir_query_and_images = \"query_and_images_keypoints_descriptions.pt\"\n",
    "torch.save(description_all, output_dir_query_and_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 30, 128])\n",
      "torch.Size([140, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "# Load descriptions of the images\n",
    "images_description = torch.load(output_dir_images)\n",
    "query_description = torch.load(output_dir_query)\n",
    "query_and_images_description = torch.load(output_dir_query_and_images)\n",
    "\n",
    "print(query_description.shape)\n",
    "print(images_description.shape)\n",
    "print(query_and_images_description.shape)\n",
    "\n",
    "#print(query_description)\n",
    "#print(query_and_images_description)\n",
    "#print(images_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one keypoint matching: Compute the cost matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "sim_matrix = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "        # Hungarian: one-to-one matching\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(np.copy(cost_matrix))\n",
    "        \n",
    "        for m in indexes:\n",
    "                sim_matrix[qkeypoint, images_keypoint] += np.exp(-cost_matrix[m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.09582365 15.95460675 15.27731852 ...  8.89354472  8.96293825\n",
      "   8.82521085]\n",
      " [ 9.52089546  9.51881673  9.47644828 ...  9.50603487  9.55114158\n",
      "   9.23158531]\n",
      " [ 9.74975131  9.79641903  9.62664098 ...  9.076581    9.04206852\n",
      "   9.21711886]\n",
      " ...\n",
      " [ 8.86574528  8.90869303  8.75763617 ...  9.09690092  9.06312987\n",
      "   9.38135474]\n",
      " [ 8.96228402  9.13603875  8.76421999 ...  9.02715686  9.07895744\n",
      "   9.10371407]\n",
      " [ 9.13884786  8.9311002   8.84456485 ... 10.18426631 17.81534871\n",
      "  11.70028291]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "one_to_one_similitude_tensor = torch.as_tensor(sim_matrix)\n",
    "\n",
    "print(one_to_one_similitude_tensor.shape)\n",
    "print(one_to_one_similitude_tensor)\n",
    "torch.save(one_to_one_similitude_tensor, \"one_to_one_similitude_matrix.pt\")\n",
    "test = torch.load(\"one_to_one_similitude_matrix.pt\")\n",
    "print(test.shape)\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to many matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_many_to_many = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "\n",
    "        sim_matrix = np.exp(-cost_matrix)\n",
    "        x = sim_matrix/ np.linalg.norm(sim_matrix, axis=0)\n",
    "        sim_matrix_many_to_many[qkeypoint][images_keypoint] = np.multiply(sim_matrix, x).sum()\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"many_to_many_similitude_matrix\", sim_matrix_many_to_many, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51.7252415  50.62483538 51.34427346 ... 44.00673066 43.55393212\n",
      "  43.21341456]\n",
      " [46.51604753 46.30991434 46.43907204 ... 47.23570939 46.66816236\n",
      "  45.44350613]\n",
      " [45.06540521 45.20175599 45.25944013 ... 44.84995313 44.51627118\n",
      "  44.35296605]\n",
      " ...\n",
      " [43.30583658 43.58107569 43.37359059 ... 44.64019902 44.25281825\n",
      "  44.79963042]\n",
      " [44.18130976 44.43467499 43.74936925 ... 44.90966726 44.62806266\n",
      "  44.80982697]\n",
      " [44.51699584 43.99887962 43.8592617  ... 48.92385104 52.4003462\n",
      "  47.40526241]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#print(sim_matrix_many_to_many[1])\n",
    "print(sim_matrix_many_to_many)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "sim_matrix_many_to_many_tensor = torch.as_tensor(sim_matrix_many_to_many)\n",
    "\n",
    "print(sim_matrix_many_to_many_tensor.shape)\n",
    "print(sim_matrix_many_to_many_tensor)\n",
    "torch.save(sim_matrix_many_to_many_tensor, \"many_to_many_similitude_matrix.pt\")\n",
    "test2 = torch.load(\"many_to_many_similitude_matrix.pt\")\n",
    "print(test2.shape)\n",
    "print(test2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
