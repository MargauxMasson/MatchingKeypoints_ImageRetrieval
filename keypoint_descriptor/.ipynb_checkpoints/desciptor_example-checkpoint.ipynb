{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE pretrained network to output keypoint's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptor_CNN3 import DesNet\n",
    "model = DesNet()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "weight_path = \"checkpoint.pth\"\n",
    "trained_weight = torch.load(weight_path)\n",
    "model.load_state_dict(trained_weight['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 1, 32, 32])\n",
      "torch.Size([35, 30, 1, 32, 32])\n",
      "torch.Size([175, 30, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# load patches\n",
    "patches_dir_images = \"../keypoint_detector/patches_images.pt\"\n",
    "patches_dir_query = \"../keypoint_detector/patches_query.pt\"\n",
    "patches_dir_all = \"../keypoint_detector/patches_all.pt\"\n",
    "patches_images = torch.load(patches_dir_images)\n",
    "patches_query = torch.load(patches_dir_query)\n",
    "patches_all = torch.load(patches_dir_all)\n",
    "\n",
    "print(patches_images.shape)\n",
    "print(patches_query.shape)\n",
    "print(patches_all.shape)\n",
    "\n",
    "patches_query =  patches_query.view(-1, 1, 32, 32).cuda()\n",
    "patches_images =  patches_images.view(-1, 1, 32, 32).cuda()\n",
    "patches_all =  patches_all.view(-1, 1, 32, 32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 128])\n",
      "torch.Size([35, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    description_images = model(patches_images)\n",
    "    description_images = description_images.view(-1, 30, 128).cpu().data\n",
    "    description_query = model(patches_query)\n",
    "    description_query = description_query.view(-1, 30, 128).cpu().data\n",
    "    description_all = model(patches_all)\n",
    "    description_all = description_all.view(-1, 30, 128).cpu().data\n",
    "\n",
    "    print(description_images.shape)\n",
    "    print(description_query.shape)\n",
    "    print(description_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save deep features  \n",
    "# IMAGES\n",
    "output_dir_images = \"images_keypoints_descriptions.pt\"\n",
    "torch.save(description_images, output_dir_images)\n",
    "\n",
    "# QUERY\n",
    "output_dir_query = \"query_keypoints_descriptions.pt\"\n",
    "torch.save(description_query, output_dir_query)\n",
    "\n",
    "# QUERY + IMAGES\n",
    "output_dir_query_and_images = \"query_and_images_keypoints_descriptions.pt\"\n",
    "torch.save(description_all, output_dir_query_and_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 30, 128])\n",
      "torch.Size([140, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "# Load descriptions of the images\n",
    "images_description = torch.load(output_dir_images)\n",
    "query_description = torch.load(output_dir_query)\n",
    "query_and_images_description = torch.load(output_dir_query_and_images)\n",
    "\n",
    "print(query_description.shape)\n",
    "print(images_description.shape)\n",
    "print(query_and_images_description.shape)\n",
    "\n",
    "#print(query_description)\n",
    "#print(query_and_images_description)\n",
    "#print(images_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one keypoint matching: Compute the cost matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "sim_matrix = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "        # Hungarian: one-to-one matching\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(np.copy(cost_matrix))\n",
    "        \n",
    "        for m in indexes:\n",
    "                sim_matrix[qkeypoint, images_keypoint] += np.exp(-cost_matrix[m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.09582365 15.95460675 15.27731852 ...  8.89354472  8.96293825\n",
      "   8.82521085]\n",
      " [ 9.52089546  9.51881673  9.47644828 ...  9.50603487  9.55114158\n",
      "   9.23158531]\n",
      " [ 9.74975131  9.79641903  9.62664098 ...  9.076581    9.04206852\n",
      "   9.21711886]\n",
      " ...\n",
      " [ 8.86574528  8.90869303  8.75763617 ...  9.09690092  9.06312987\n",
      "   9.38135474]\n",
      " [ 8.96228402  9.13603875  8.76421999 ...  9.02715686  9.07895744\n",
      "   9.10371407]\n",
      " [ 9.13884786  8.9311002   8.84456485 ... 10.18426631 17.81534871\n",
      "  11.70028291]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "one_to_one_similitude_tensor = torch.as_tensor(sim_matrix)\n",
    "\n",
    "print(one_to_one_similitude_tensor.shape)\n",
    "print(one_to_one_similitude_tensor)\n",
    "torch.save(one_to_one_similitude_tensor, \"one_to_one_similitude_matrix.pt\")\n",
    "test = torch.load(\"one_to_one_similitude_matrix.pt\")\n",
    "print(test.shape)\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to many matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_many_to_many = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "\n",
    "        sim_matrix = np.exp(-cost_matrix)\n",
    "        x = sim_matrix/ np.linalg.norm(sim_matrix, axis=0)\n",
    "        sim_matrix_many_to_many[qkeypoint][images_keypoint] = np.multiply(sim_matrix, x).sum()\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"many_to_many_similitude_matrix\", sim_matrix_many_to_many, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46.51604753 46.30991434 46.43907204 45.17328869 56.45048011 52.19327766\n",
      " 56.46916245 51.79258474 45.50984654 47.6321715  46.72387101 46.78676133\n",
      " 47.22453235 47.40653367 46.87159985 47.17905628 48.28910455 47.3317964\n",
      " 48.90385503 49.75491219 46.28111984 46.00076089 45.63235056 48.13291171\n",
      " 47.72901656 46.82686932 49.85531572 49.24961306 44.6640847  44.89421274\n",
      " 46.49544776 44.46259145 49.79125604 51.50935388 48.20145363 49.90825174\n",
      " 45.64379699 45.44420931 45.24038101 45.82500719 46.45355844 46.17245942\n",
      " 46.86584569 47.06756172 44.58949031 45.1247245  43.90352714 45.78206475\n",
      " 48.00824357 46.69227198 47.76317193 48.47168629 51.03310476 49.36221089\n",
      " 52.48793555 52.23334088 47.54096715 47.63102378 48.14272232 46.79033503\n",
      " 48.49735084 48.43731566 48.61587605 47.20785493 43.4772903  45.51475311\n",
      " 43.17648394 43.11120899 45.08322703 45.19922376 44.78336653 47.58938841\n",
      " 48.94379805 49.81556589 49.29652192 50.29329307 46.28458867 46.19479543\n",
      " 46.90088283 46.76183257 46.05190552 47.08512951 47.30350663 46.43184961\n",
      " 46.9146198  47.47014243 47.48229461 47.11230743 46.84510421 46.95338477\n",
      " 47.18287266 47.71241112 45.25085262 45.89391992 46.95846767 47.24907176\n",
      " 45.08963276 44.43693242 43.05709815 43.57912802 43.50426664 43.03281711\n",
      " 44.53402667 44.26396409 46.60321838 45.89646357 47.18838483 45.51823618\n",
      " 46.86242162 47.21018544 46.46621997 46.40349867 45.65886949 44.80495956\n",
      " 44.88733092 46.04773122 47.65518816 50.16692681 49.69156095 49.77273068\n",
      " 45.675952   44.53877107 45.31215403 45.94266525 46.96385905 45.84106486\n",
      " 46.71458487 46.39066031 48.01473619 46.08077759 46.72203052 47.20320974\n",
      " 47.07806696 47.44952854 49.28050255 47.12574025 45.99013652 47.23570939\n",
      " 46.66816236 45.44350613]\n"
     ]
    }
   ],
   "source": [
    "#print(sim_matrix_many_to_many[1])\n",
    "print(sim_matrix_many_to_many)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "sim_matrix_many_to_many_tensor = torch.as_tensor(sim_matrix_many_to_many)\n",
    "\n",
    "print(sim_matrix_many_to_many_tensor.shape)\n",
    "print(sim_matrix_many_to_many_tensor)\n",
    "torch.save(sim_matrix_many_to_many_tensor, \"one_to_one_similitude_matrix.pt\")\n",
    "test2 = torch.load(\"sim_matrix_many_to_many_matrix.pt\")\n",
    "print(test2.shape)\n",
    "print(test2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
