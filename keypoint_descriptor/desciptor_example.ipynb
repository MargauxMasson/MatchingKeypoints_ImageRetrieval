{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE pretrained network to output keypoint's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptor_CNN3 import DesNet\n",
    "model = DesNet()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "weight_path = \"checkpoint.pth\"\n",
    "trained_weight = torch.load(weight_path)\n",
    "model.load_state_dict(trained_weight['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 1, 32, 32])\n",
      "torch.Size([35, 30, 1, 32, 32])\n",
      "torch.Size([175, 30, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# load patches\n",
    "patches_dir_images = \"../keypoint_detector/patches_images.pt\"\n",
    "patches_dir_query = \"../keypoint_detector/patches_query.pt\"\n",
    "patches_dir_all = \"../keypoint_detector/patches_all.pt\"\n",
    "patches_images = torch.load(patches_dir_images)\n",
    "patches_query = torch.load(patches_dir_query)\n",
    "patches_all = torch.load(patches_dir_all)\n",
    "\n",
    "print(patches_images.shape)\n",
    "print(patches_query.shape)\n",
    "print(patches_all.shape)\n",
    "\n",
    "patches_query =  patches_query.view(-1, 1, 32, 32).cuda()\n",
    "patches_images =  patches_images.view(-1, 1, 32, 32).cuda()\n",
    "patches_all =  patches_all.view(-1, 1, 32, 32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 128])\n",
      "torch.Size([35, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    description_images = model(patches_images)\n",
    "    description_images = description_images.view(-1, 30, 128).cpu().data\n",
    "    description_query = model(patches_query)\n",
    "    description_query = description_query.view(-1, 30, 128).cpu().data\n",
    "    description_all = model(patches_all)\n",
    "    description_all = description_all.view(-1, 30, 128).cpu().data\n",
    "\n",
    "    print(description_images.shape)\n",
    "    print(description_query.shape)\n",
    "    print(description_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save deep features  \n",
    "# IMAGES\n",
    "output_dir_images = \"images_keypoints_descriptions.pt\"\n",
    "torch.save(description_images, output_dir_images)\n",
    "\n",
    "# QUERY\n",
    "output_dir_query = \"query_keypoints_descriptions.pt\"\n",
    "torch.save(description_query, output_dir_query)\n",
    "\n",
    "# QUERY + IMAGES\n",
    "output_dir_query_and_images = \"query_and_images_keypoints_descriptions.pt\"\n",
    "torch.save(description_all, output_dir_query_and_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 30, 128])\n",
      "torch.Size([140, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "# Load descriptions of the images\n",
    "images_description = torch.load(output_dir_images)\n",
    "query_description = torch.load(output_dir_query)\n",
    "query_and_images_description = torch.load(output_dir_query_and_images)\n",
    "\n",
    "print(query_description.shape)\n",
    "print(images_description.shape)\n",
    "print(query_and_images_description.shape)\n",
    "\n",
    "#print(query_description)\n",
    "#print(query_and_images_description)\n",
    "#print(images_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one keypoint matching: Compute the cost matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "sim_matrix = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "        # Hungarian: one-to-one matching\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(np.copy(cost_matrix))\n",
    "        \n",
    "        for m in indexes:\n",
    "                sim_matrix[qkeypoint, images_keypoint] += np.exp(-cost_matrix[m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.09582365 15.95460675 15.27731852 ...  8.89354472  8.96293825\n",
      "   8.82521085]\n",
      " [ 9.52089546  9.51881673  9.47644828 ...  9.50603487  9.55114158\n",
      "   9.23158531]\n",
      " [ 9.74975131  9.79641903  9.62664098 ...  9.076581    9.04206852\n",
      "   9.21711886]\n",
      " ...\n",
      " [ 8.86574528  8.90869303  8.75763617 ...  9.09690092  9.06312987\n",
      "   9.38135474]\n",
      " [ 8.96228402  9.13603875  8.76421999 ...  9.02715686  9.07895744\n",
      "   9.10371407]\n",
      " [ 9.13884786  8.9311002   8.84456485 ... 10.18426631 17.81534871\n",
      "  11.70028291]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "one_to_one_similitude_tensor = torch.as_tensor(sim_matrix)\n",
    "\n",
    "print(one_to_one_similitude_tensor.shape)\n",
    "print(one_to_one_similitude_tensor)\n",
    "torch.save(one_to_one_similitude_tensor, \"one_to_one_similitude_matrix.pt\")\n",
    "test = torch.load(\"one_to_one_similitude_matrix.pt\")\n",
    "print(test.shape)\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to many matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_many_to_many = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "\n",
    "        sim_matrix = np.exp(-cost_matrix)\n",
    "        x = sim_matrix/ np.linalg.norm(sim_matrix, axis=0)\n",
    "        sim_matrix_many_to_many[qkeypoint][images_keypoint] = np.multiply(sim_matrix, x).sum()\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"many_to_many_similitude_matrix\", sim_matrix_many_to_many, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51.7252415  50.62483538 51.34427346 ... 44.00673066 43.55393212\n",
      "  43.21341456]\n",
      " [46.51604753 46.30991434 46.43907204 ... 47.23570939 46.66816236\n",
      "  45.44350613]\n",
      " [45.06540521 45.20175599 45.25944013 ... 44.84995313 44.51627118\n",
      "  44.35296605]\n",
      " ...\n",
      " [43.30583658 43.58107569 43.37359059 ... 44.64019902 44.25281825\n",
      "  44.79963042]\n",
      " [44.18130976 44.43467499 43.74936925 ... 44.90966726 44.62806266\n",
      "  44.80982697]\n",
      " [44.51699584 43.99887962 43.8592617  ... 48.92385104 52.4003462\n",
      "  47.40526241]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#print(sim_matrix_many_to_many[1])\n",
    "print(sim_matrix_many_to_many)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "sim_matrix_many_to_many_tensor = torch.as_tensor(sim_matrix_many_to_many)\n",
    "\n",
    "print(sim_matrix_many_to_many_tensor.shape)\n",
    "print(sim_matrix_many_to_many_tensor)\n",
    "torch.save(sim_matrix_many_to_many_tensor, \"many_to_many_similitude_matrix.pt\")\n",
    "test2 = torch.load(\"many_to_many_similitude_matrix.pt\")\n",
    "print(test2.shape)\n",
    "print(test2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 140])\n",
      "140\n",
      "['q1' 'q1' 'q1' 'q1' 'q2' 'q2' 'q2' 'q2' 'q3' 'q3' 'q3' 'q3' 'q4' 'q4'\n",
      " 'q4' 'q4' 'q5' 'q5' 'q5' 'q5' 'q6' 'q6' 'q6' 'q6' 'q7' 'q7' 'q7' 'q7'\n",
      " 'q8' 'q8' 'q8' 'q8' 'q9' 'q9' 'q9' 'q9' 'q10' 'q10' 'q10' 'q10' 'q11'\n",
      " 'q11' 'q11' 'q11' 'q12' 'q12' 'q12' 'q12' 'q13' 'q13' 'q13' 'q13' 'q14'\n",
      " 'q14' 'q14' 'q14' 'q15' 'q15' 'q15' 'q15' 'q16' 'q16' 'q16' 'q16' 'q17'\n",
      " 'q17' 'q17' 'q17' 'q18' 'q18' 'q18' 'q18' 'q19' 'q19' 'q19' 'q19' 'q20'\n",
      " 'q20' 'q20' 'q20' 'q21' 'q21' 'q21' 'q21' 'q22' 'q22' 'q22' 'q22' 'q23'\n",
      " 'q23' 'q23' 'q23' 'q24' 'q24' 'q24' 'q24' 'q25' 'q25' 'q25' 'q25' 'q26'\n",
      " 'q26' 'q26' 'q26' 'q27' 'q27' 'q27' 'q27' 'q28' 'q28' 'q28' 'q28' 'q29'\n",
      " 'q29' 'q29' 'q29' 'q30' 'q30' 'q30' 'q30' 'q31' 'q31' 'q31' 'q31' 'q32'\n",
      " 'q32' 'q32' 'q32' 'q33' 'q33' 'q33' 'q33' 'q34' 'q34' 'q34' 'q34' 'q35'\n",
      " 'q35' 'q35' 'q35']\n",
      "140\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate keypoint matching by estimating precision and recall of image retrieval\n",
    "\n",
    "print(one_to_one_similitude_matrix.shape)\n",
    "queryIDs, indexes_sim_matrix = np.loadtxt(\"../image_retrieval/ground_truth.txt\", dtype={'names': ['label', 'age'],'formats': ('U10', 'i4')}, usecols =(0, 1), unpack = True)\n",
    "\n",
    "print(queryIDs.size)\n",
    "print(queryIDs)\n",
    "print(indexes_sim_matrix.size)\n",
    "print(indexes_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "[[16.09582365 15.95460675 15.27731852 10.70013843]\n",
      " [18.35905365 12.01519314 17.02791084 11.22082701]\n",
      " [11.19231394  9.8221744  10.07631916 10.36073284]\n",
      " [10.10793091 11.81372457 10.20200618 10.30718051]\n",
      " [10.81465856 10.38381926 13.91225466 10.72107825]\n",
      " [11.0145072  10.57364117 13.74681125 10.10929196]\n",
      " [ 9.84762747  9.78008679 11.56229506 10.28114252]\n",
      " [11.86789712  9.25485534  9.87121722  9.67156965]\n",
      " [10.01182914 10.22950596 13.94898707 10.16717246]\n",
      " [10.63174413 11.07118195 10.63186327 11.86184171]\n",
      " [11.80982889 10.84509129 14.13657084 10.68949773]\n",
      " [10.65856797  9.58412292  8.96423357 10.62006602]\n",
      " [10.57457276 10.15286861 11.13359404 12.87035559]\n",
      " [11.73474829 11.19408156 17.20939132 17.6279689 ]\n",
      " [13.0539603  12.00582499 12.36519144 11.28304994]\n",
      " [12.06723036 13.94694391 12.09248969 11.69938223]\n",
      " [ 7.17936776  7.22924833  7.18721352  7.15057108]\n",
      " [ 9.36335604 10.2377293  10.30495244  9.81683454]\n",
      " [12.0857364  11.51166488 10.7676409  10.6381713 ]\n",
      " [10.64134566 10.08928739 10.32301853 10.9389098 ]\n",
      " [10.00797599 10.09110489 10.54538629 10.79704455]\n",
      " [11.21647714 10.15359563 10.27024805 10.12172459]\n",
      " [12.79171633 11.94857513 14.51466025 10.82868614]\n",
      " [15.02598765  9.22614779  9.28885518  9.68700729]\n",
      " [14.1709297  10.53355413  9.28417072 10.24743185]\n",
      " [11.02759107 11.16917424 10.19862166 10.20007474]\n",
      " [10.30644206  9.57160094 10.31038263  9.45012546]\n",
      " [10.88622806 10.52080097 11.20404135 10.01333562]\n",
      " [13.75759668 10.43257968 12.95551338  9.5195153 ]\n",
      " [10.92297686 10.4479584  10.30438821 10.69698757]\n",
      " [10.93797162  9.81927239 14.70787169 10.72873799]\n",
      " [ 9.84388549  9.73491643  9.84148274  9.55378891]\n",
      " [10.90501484 10.64002577  9.47144966  9.87925607]\n",
      " [12.17107014 11.54335714 11.58775402 10.71448506]\n",
      " [13.82280852 10.18426631 17.81534871 11.70028291]]\n",
      "140\n",
      "[[51.7252415  50.62483538 51.34427346 46.35857927]\n",
      " [56.45048011 52.19327766 56.46916245 51.79258474]\n",
      " [46.50348156 46.05835924 46.60637273 45.97748304]\n",
      " [46.83611145 48.66946421 46.8843311  47.402617  ]\n",
      " [48.17262216 47.54866329 50.96001388 48.9228074 ]\n",
      " [47.4911121  45.68763036 50.09996204 47.94723942]\n",
      " [47.45926302 46.47337593 50.0871456  47.92347474]\n",
      " [46.64175925 45.23169008 45.29838318 45.48974221]\n",
      " [47.40161984 47.69339331 49.90526006 47.54492935]\n",
      " [48.98479373 48.62911527 48.14666093 49.11872587]\n",
      " [48.78629472 49.12788596 51.10762782 47.85300418]\n",
      " [45.87789307 45.02662292 42.68701096 46.16076465]\n",
      " [49.03499116 47.89625544 48.44282847 50.93454111]\n",
      " [51.30800817 49.61293279 57.1356107  56.71501722]\n",
      " [50.76434531 49.27348972 50.55786361 49.21882775]\n",
      " [50.71427729 51.8096207  51.38963139 51.18673812]\n",
      " [39.32301604 39.5962235  39.36598949 39.16529031]\n",
      " [44.64608687 46.5111049  46.43739794 46.07527687]\n",
      " [51.37618172 52.48972043 50.36661522 51.16574766]\n",
      " [48.34485084 47.81442402 49.16052277 50.07652639]\n",
      " [47.30944105 47.80031121 47.94155193 48.13432775]\n",
      " [48.86430199 47.26521081 46.88942511 47.22986273]\n",
      " [49.41648767 48.70137189 50.49534216 49.1755688 ]\n",
      " [53.39976996 44.05197806 45.92775048 46.32196647]\n",
      " [67.83362803 53.2017645  47.6908035  51.34904952]\n",
      " [46.86010975 46.81307447 46.71183781 46.55185651]\n",
      " [46.27986369 45.77591446 48.00344275 45.36399198]\n",
      " [46.81403479 46.80865934 47.13257379 45.76065923]\n",
      " [48.77579874 45.30348637 48.4572379  44.91820655]\n",
      " [50.66361934 52.14576499 52.88343006 52.6435822 ]\n",
      " [46.92453086 44.31017612 50.36430586 46.72976058]\n",
      " [47.53996035 46.68645724 46.7059656  46.29606179]\n",
      " [47.42099582 47.84970186 46.61390857 47.01682433]\n",
      " [51.95637981 51.42943061 51.70499347 49.26791686]\n",
      " [48.97948734 48.92385104 52.4003462  47.40526241]]\n"
     ]
    }
   ],
   "source": [
    "one_to_one_similitude_matrix = torch.load(\"results/one_to_one_similitude_matrix.pt\")\n",
    "many_to_many_similitude_matrix = torch.load(\"results/many_to_many_similitude_matrix.pt\")\n",
    "\n",
    "# K = most similar images\n",
    "K_one_to_one = np.zeros((35,4))\n",
    "K_many_to_many = np.zeros((35,4))\n",
    "#print(K)\n",
    "id = 0\n",
    "for i in range(queryIDs.size):\n",
    "    #print(queryIDs[i])\n",
    "    for queryID in range(36):\n",
    "        #id = 0\n",
    "        if queryIDs[i] == 'q'+ str(queryID):\n",
    "           # print(\"hello \" + 'q'+  str(queryID))\n",
    "            m = indexes_sim_matrix[i]\n",
    "           # print(m)\n",
    "            sim_one_to_one = one_to_one_similitude_matrix[queryID-1][i].data.cpu().numpy()\n",
    "            sim_many_to_many = many_to_many_similitude_matrix[queryID-1][i].data.cpu().numpy()\n",
    "            #print(sim)\n",
    "            K_one_to_one[queryID-1][id] = sim_one_to_one\n",
    "            K_many_to_many[queryID-1][id] = sim_many_to_many\n",
    "            id += 1\n",
    "        if id == 4:\n",
    "            id = 0\n",
    "            \n",
    "print(K_one_to_one.size)\n",
    "print(K_one_to_one)\n",
    "\n",
    "print(K_many_to_many.size)\n",
    "print(K_many_to_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine true or false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "112\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "K_one_to_one_positives = np.zeros((35,4))\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "for i in range(35):\n",
    "    for j in range(4):\n",
    "        if K_one_to_one[i][j] > 10:\n",
    "            #print(K_one_to_one[i][j])\n",
    "            K_one_to_one_positives[i][j] = 1\n",
    "            true_positives += 1\n",
    "        else: \n",
    "            K_one_to_one_positives[i][j] = 0\n",
    "            false_positives += 1\n",
    "\n",
    "print(K_one_to_one_positives)\n",
    "print(true_positives)\n",
    "print(false_positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "40\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "K_many_to_many_positives = np.zeros((35,4))\n",
    "true_many_to_many_positives = 0\n",
    "false_many_to_many_positives = 0\n",
    "for i in range(35):\n",
    "    for j in range(4):\n",
    "        if K_many_to_many[i][j] > 50:\n",
    "            #print(K_one_to_one[i][j])\n",
    "            K_many_to_many_positives[i][j] = 1\n",
    "            true_many_to_many_positives += 1\n",
    "        else: \n",
    "            K_many_to_many_positives[i][j] = 0\n",
    "            false_many_to_many_positives += 1\n",
    "\n",
    "print(K_many_to_many_positives)\n",
    "print(true_many_to_many_positives)\n",
    "print(false_many_to_many_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
