{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE pretrained network to output keypoint's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptor_CNN3 import DesNet\n",
    "model = DesNet()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "weight_path = \"checkpoint.pth\"\n",
    "trained_weight = torch.load(weight_path)\n",
    "model.load_state_dict(trained_weight['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 1, 32, 32])\n",
      "torch.Size([35, 30, 1, 32, 32])\n",
      "torch.Size([175, 30, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# load patches\n",
    "patches_dir_images = \"../keypoint_detector/patches_images.pt\"\n",
    "patches_dir_query = \"../keypoint_detector/patches_query.pt\"\n",
    "patches_dir_all = \"../keypoint_detector/patches_all.pt\"\n",
    "patches_images = torch.load(patches_dir_images)\n",
    "patches_query = torch.load(patches_dir_query)\n",
    "patches_all = torch.load(patches_dir_all)\n",
    "\n",
    "print(patches_images.shape)\n",
    "print(patches_query.shape)\n",
    "print(patches_all.shape)\n",
    "\n",
    "patches_query =  patches_query.view(-1, 1, 32, 32).cuda()\n",
    "patches_images =  patches_images.view(-1, 1, 32, 32).cuda()\n",
    "patches_all =  patches_all.view(-1, 1, 32, 32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 128])\n",
      "torch.Size([35, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    description_images = model(patches_images)\n",
    "    description_images = description_images.view(-1, 30, 128).cpu().data\n",
    "    description_query = model(patches_query)\n",
    "    description_query = description_query.view(-1, 30, 128).cpu().data\n",
    "    description_all = model(patches_all)\n",
    "    description_all = description_all.view(-1, 30, 128).cpu().data\n",
    "\n",
    "    print(description_images.shape)\n",
    "    print(description_query.shape)\n",
    "    print(description_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save deep features  \n",
    "# IMAGES\n",
    "output_dir_images = \"images_keypoints_descriptions.pt\"\n",
    "torch.save(description_images, output_dir_images)\n",
    "\n",
    "# QUERY\n",
    "output_dir_query = \"query_keypoints_descriptions.pt\"\n",
    "torch.save(description_query, output_dir_query)\n",
    "\n",
    "# QUERY + IMAGES\n",
    "output_dir_query_and_images = \"query_and_images_keypoints_descriptions.pt\"\n",
    "torch.save(description_all, output_dir_query_and_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 30, 128])\n",
      "torch.Size([140, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "# Load descriptions of the images\n",
    "images_description = torch.load(output_dir_images)\n",
    "query_description = torch.load(output_dir_query)\n",
    "query_and_images_description = torch.load(output_dir_query_and_images)\n",
    "\n",
    "print(query_description.shape)\n",
    "print(images_description.shape)\n",
    "print(query_and_images_description.shape)\n",
    "\n",
    "#print(query_description)\n",
    "#print(query_and_images_description)\n",
    "#print(images_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one keypoint matching: Compute the cost matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "sim_matrix = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "        # Hungarian: one-to-one matching\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(np.copy(cost_matrix))\n",
    "        \n",
    "        for m in indexes:\n",
    "                sim_matrix[qkeypoint, images_keypoint] += np.exp(-cost_matrix[m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.09582365 15.95460675 15.27731852 ...  8.89354472  8.96293825\n",
      "   8.82521085]\n",
      " [ 9.52089546  9.51881673  9.47644828 ...  9.50603487  9.55114158\n",
      "   9.23158531]\n",
      " [ 9.74975131  9.79641903  9.62664098 ...  9.076581    9.04206852\n",
      "   9.21711886]\n",
      " ...\n",
      " [ 8.86574528  8.90869303  8.75763617 ...  9.09690092  9.06312987\n",
      "   9.38135474]\n",
      " [ 8.96228402  9.13603875  8.76421999 ...  9.02715686  9.07895744\n",
      "   9.10371407]\n",
      " [ 9.13884786  8.9311002   8.84456485 ... 10.18426631 17.81534871\n",
      "  11.70028291]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[16.0958, 15.9546, 15.2773,  ...,  8.8935,  8.9629,  8.8252],\n",
      "        [ 9.5209,  9.5188,  9.4764,  ...,  9.5060,  9.5511,  9.2316],\n",
      "        [ 9.7498,  9.7964,  9.6266,  ...,  9.0766,  9.0421,  9.2171],\n",
      "        ...,\n",
      "        [ 8.8657,  8.9087,  8.7576,  ...,  9.0969,  9.0631,  9.3814],\n",
      "        [ 8.9623,  9.1360,  8.7642,  ...,  9.0272,  9.0790,  9.1037],\n",
      "        [ 9.1388,  8.9311,  8.8446,  ..., 10.1843, 17.8153, 11.7003]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "one_to_one_similitude_tensor = torch.as_tensor(sim_matrix)\n",
    "\n",
    "print(one_to_one_similitude_tensor.shape)\n",
    "print(one_to_one_similitude_tensor)\n",
    "torch.save(one_to_one_similitude_tensor, \"one_to_one_similitude_matrix.pt\")\n",
    "test = torch.load(\"one_to_one_similitude_matrix.pt\")\n",
    "print(test.shape)\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to many matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_many_to_many = np.zeros((35,140))\n",
    "\n",
    "for qkeypoint in range(35):\n",
    "    for images_keypoint in range(140):\n",
    "        cost_matrix = np.zeros((30, 30))\n",
    "        for i in range(30):\n",
    "            for j in range(30):\n",
    "                cost_matrix[i][j] = np.linalg.norm(query_description[qkeypoint][i].cpu().numpy() - images_description[images_keypoint][j].cpu().numpy())\n",
    "\n",
    "        sim_matrix = np.exp(-cost_matrix)\n",
    "        x = sim_matrix/ np.linalg.norm(sim_matrix, axis=0)\n",
    "        sim_matrix_many_to_many[qkeypoint][images_keypoint] = np.multiply(sim_matrix, x).sum()\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"many_to_many_similitude_matrix\", sim_matrix_many_to_many, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51.7252415  50.62483538 51.34427346 ... 44.00673066 43.55393212\n",
      "  43.21341456]\n",
      " [46.51604753 46.30991434 46.43907204 ... 47.23570939 46.66816236\n",
      "  45.44350613]\n",
      " [45.06540521 45.20175599 45.25944013 ... 44.84995313 44.51627118\n",
      "  44.35296605]\n",
      " ...\n",
      " [43.30583658 43.58107569 43.37359059 ... 44.64019902 44.25281825\n",
      "  44.79963042]\n",
      " [44.18130976 44.43467499 43.74936925 ... 44.90966726 44.62806266\n",
      "  44.80982697]\n",
      " [44.51699584 43.99887962 43.8592617  ... 48.92385104 52.4003462\n",
      "  47.40526241]]\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([35, 140])\n",
      "tensor([[51.7252, 50.6248, 51.3443,  ..., 44.0067, 43.5539, 43.2134],\n",
      "        [46.5160, 46.3099, 46.4391,  ..., 47.2357, 46.6682, 45.4435],\n",
      "        [45.0654, 45.2018, 45.2594,  ..., 44.8500, 44.5163, 44.3530],\n",
      "        ...,\n",
      "        [43.3058, 43.5811, 43.3736,  ..., 44.6402, 44.2528, 44.7996],\n",
      "        [44.1813, 44.4347, 43.7494,  ..., 44.9097, 44.6281, 44.8098],\n",
      "        [44.5170, 43.9989, 43.8593,  ..., 48.9239, 52.4003, 47.4053]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#print(sim_matrix_many_to_many[1])\n",
    "print(sim_matrix_many_to_many)\n",
    "#np.savetxt(\"one_to_one_similitude_matrix\", sim_matrix, delimiter=\",\")\n",
    "sim_matrix_many_to_many_tensor = torch.as_tensor(sim_matrix_many_to_many)\n",
    "\n",
    "print(sim_matrix_many_to_many_tensor.shape)\n",
    "print(sim_matrix_many_to_many_tensor)\n",
    "torch.save(sim_matrix_many_to_many_tensor, \"many_to_many_similitude_matrix.pt\")\n",
    "test2 = torch.load(\"many_to_many_similitude_matrix.pt\")\n",
    "print(test2.shape)\n",
    "print(test2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 140])\n",
      "140\n",
      "['q1' 'q1' 'q1' 'q1' 'q2' 'q2' 'q2' 'q2' 'q3' 'q3' 'q3' 'q3' 'q4' 'q4'\n",
      " 'q4' 'q4' 'q5' 'q5' 'q5' 'q5' 'q6' 'q6' 'q6' 'q6' 'q7' 'q7' 'q7' 'q7'\n",
      " 'q8' 'q8' 'q8' 'q8' 'q9' 'q9' 'q9' 'q9' 'q10' 'q10' 'q10' 'q10' 'q11'\n",
      " 'q11' 'q11' 'q11' 'q12' 'q12' 'q12' 'q12' 'q13' 'q13' 'q13' 'q13' 'q14'\n",
      " 'q14' 'q14' 'q14' 'q15' 'q15' 'q15' 'q15' 'q16' 'q16' 'q16' 'q16' 'q17'\n",
      " 'q17' 'q17' 'q17' 'q18' 'q18' 'q18' 'q18' 'q19' 'q19' 'q19' 'q19' 'q20'\n",
      " 'q20' 'q20' 'q20' 'q21' 'q21' 'q21' 'q21' 'q22' 'q22' 'q22' 'q22' 'q23'\n",
      " 'q23' 'q23' 'q23' 'q24' 'q24' 'q24' 'q24' 'q25' 'q25' 'q25' 'q25' 'q26'\n",
      " 'q26' 'q26' 'q26' 'q27' 'q27' 'q27' 'q27' 'q28' 'q28' 'q28' 'q28' 'q29'\n",
      " 'q29' 'q29' 'q29' 'q30' 'q30' 'q30' 'q30' 'q31' 'q31' 'q31' 'q31' 'q32'\n",
      " 'q32' 'q32' 'q32' 'q33' 'q33' 'q33' 'q33' 'q34' 'q34' 'q34' 'q34' 'q35'\n",
      " 'q35' 'q35' 'q35']\n",
      "140\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate keypoint matching by estimating precision and recall of image retrieval\n",
    "\n",
    "print(one_to_one_similitude_matrix.shape)\n",
    "queryIDs, indexes_sim_matrix = np.loadtxt(\"../image_retrieval/ground_truth.txt\", dtype={'names': ['label', 'age'],'formats': ('U10', 'i4')}, usecols =(0, 1), unpack = True)\n",
    "\n",
    "print(queryIDs.size)\n",
    "print(queryIDs)\n",
    "print(indexes_sim_matrix.size)\n",
    "print(indexes_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_images(similitude_matrix, K): \n",
    "    queryIDs, indexes_sim_matrix = np.loadtxt(\"../image_retrieval/ground_truth.txt\", dtype={'names': ['label', 'age'],'formats': ('U10', 'i4')}, usecols =(0, 1), unpack = True)\n",
    "    K_matrix_most_similar_images = np.zeros((35,K))\n",
    "    id = 0\n",
    "    for i in range(queryIDs.size):\n",
    "        for queryID in range(36):\n",
    "            if queryIDs[i] == 'q'+ str(queryID):\n",
    "                m = indexes_sim_matrix[i]\n",
    "                K_matrix_most_similar_images[queryID-1][id] = similitude_matrix[queryID-1][i]\n",
    "                id += 1\n",
    "            if id == K:\n",
    "                id = 0\n",
    "    return K_matrix_most_similar_images\n",
    "            \n",
    "#print(most_similar_images(one_to_one_similitude_matrix, 4))\n",
    "#print(most_similar_images(many_to_many_similitude_matrix, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine true or false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nmaxelements(list1, N): \n",
    "    l = list1.copy()\n",
    "    final_list = []\n",
    "  \n",
    "    for i in range(0, N):  \n",
    "        max1 = 0\n",
    "          \n",
    "        for j in range(len(list1)):      \n",
    "            if list1[j] > max1: \n",
    "                max1 = list1[j];\n",
    "                list1[j] = 0;\n",
    "\n",
    "        final_list.append(max1)\n",
    "           \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37142857142857144, 0.2785714285714286]\n"
     ]
    }
   ],
   "source": [
    "# Determine precision and recall\n",
    "def precision_and_recall_matching(similitude_matrix, K): \n",
    "    similitude_matrix = similitude_matrix.copy()\n",
    "    highest_matching_points = np.zeros((35,K))\n",
    "    true_positives = np.zeros((35,K))\n",
    "    precision_matrix = np.zeros((35,1))\n",
    "    recall_matrix = np.zeros((35,1))\n",
    "    K_matrix_most_similar_images = most_similar_images(similitude_matrix, K)\n",
    "        \n",
    "    for i in range(35):\n",
    "            highest_matching_points[i] = Nmaxelements(similitude_matrix[i], K)\n",
    "\n",
    "    for i in range(35):\n",
    "        K_matrix_most_similar_images[i].sort();\n",
    "        highest_matching_points[i].sort()\n",
    "        total_true_positives = 0\n",
    "        for j in range(K):\n",
    "            if K_matrix_most_similar_images[i][j] == highest_matching_points[i][j]:\n",
    "                true_positives[i][j] = 1.\n",
    "                total_true_positives += 1\n",
    "            else:\n",
    "                true_positives[i][j] = 0.\n",
    "        precision_matrix[i] = total_true_positives/K\n",
    "        recall_matrix[i] = total_true_positives/4\n",
    "\n",
    "    average_precision = 0\n",
    "    average_recall = 0\n",
    "    for i in range(35):\n",
    "        average_precision += precision_matrix[i]\n",
    "        average_recall += recall_matrix[i]\n",
    "\n",
    "    average_precision = average_precision / 35\n",
    "    average_recall = average_recall / 35\n",
    "    return [float(average_precision), float(average_recall)]\n",
    "\n",
    "print(precision_and_recall_matching(one_to_one_similitude_matrix, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Matching One to One:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.14285714 0.3        0.37142857 0.40714286]]\n",
      "Recall Matching One to One:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.03571429 0.15       0.27857143 0.40714286]]\n",
      "----------------------------------------------------\n",
      "Recall Matching Many to Many:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.01428571 0.11428571 0.17857143 0.28571429]]\n",
      "Precision Matching Many to Many:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.05714286 0.22857143 0.23809524 0.28571429]]\n"
     ]
    }
   ],
   "source": [
    "# Load similitude matrixes: one-to-one & many-to-many\n",
    "one_to_one_similitude_matrix = torch.load(\"results/one_to_one_similitude_matrix.pt\")\n",
    "one_to_one_similitude_matrix = one_to_one_similitude_matrix.numpy()\n",
    "many_to_many_similitude_matrix = torch.load(\"results/many_to_many_similitude_matrix.pt\")\n",
    "many_to_many_similitude_matrix = many_to_many_similitude_matrix.numpy()\n",
    "\n",
    "precision_matching_one_to_one = np.zeros((2,4))\n",
    "precision_matching_many_to_many = np.zeros((2,4))\n",
    "recall_matching_one_to_one = np.zeros((2,4))\n",
    "recall_matching_many_to_many = np.zeros((2,4))\n",
    "\n",
    "for k in range(1,5):\n",
    "    #print(precision_matching(one_to_one_similitude_matrix, k))\n",
    "    precision_matching_one_to_one[1][k-1] = precision_and_recall_matching(one_to_one_similitude_matrix, k)[0]\n",
    "    precision_matching_many_to_many[1][k-1] = precision_and_recall_matching(many_to_many_similitude_matrix, k)[0]\n",
    "    precision_matching_one_to_one[0][k-1] = k\n",
    "    precision_matching_many_to_many[0][k-1] = k\n",
    "    \n",
    "    recall_matching_one_to_one[1][k-1] = precision_and_recall_matching(one_to_one_similitude_matrix, k)[1]\n",
    "    recall_matching_many_to_many[1][k-1] = precision_and_recall_matching(many_to_many_similitude_matrix, k)[1]\n",
    "    recall_matching_one_to_one[0][k-1] = k\n",
    "    recall_matching_many_to_many[0][k-1] = k\n",
    "\n",
    "print(\"Precision Matching One to One:\")\n",
    "print(precision_matching_one_to_one)\n",
    "print(\"Recall Matching One to One:\")\n",
    "print(recall_matching_one_to_one)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Recall Matching Many to Many:\")\n",
    "print(recall_matching_many_to_many)\n",
    "print(\"Precision Matching Many to Many:\")\n",
    "print(precision_matching_many_to_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
